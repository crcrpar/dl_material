{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ref\n",
    "- [ゼロから作るDeep Learning](https://www.amazon.co.jp/%E3%82%BC%E3%83%AD%E3%81%8B%E3%82%89%E4%BD%9C%E3%82%8BDeep-Learning-Python%E3%81%A7%E5%AD%A6%E3%81%B6%E3%83%87%E3%82%A3%E3%83%BC%E3%83%97%E3%83%A9%E3%83%BC%E3%83%8B%E3%83%B3%E3%82%B0%E3%81%AE%E7%90%86%E8%AB%96%E3%81%A8%E5%AE%9F%E8%A3%85-%E6%96%8E%E8%97%A4-%E5%BA%B7%E6%AF%85/dp/4873117585/ref=sr_1_1?ie=UTF8&qid=1494484778&sr=8-1&keywords=0%E3%81%8B%E3%82%89%E4%BD%9C%E3%82%8B%E3%83%87%E3%82%A3%E3%83%BC%E3%83%97%E3%83%A9%E3%83%BC%E3%83%8B%E3%83%B3%E3%82%B0)\n",
    "- [pytorch.org](http://pytorch.org/)\n",
    "\n",
    "# パーセプトロン・活性化関数・誤差関数・誤差逆伝播\n",
    "## パーセプトロン\n",
    "パーセプトロンはニューラルネットワークの元となったモデルで、入力と出力のみからなるモデルです。\n",
    "このパーセプトロンはXORすら解けない程度でした。パーセプトロンの関数としての問題は、**線形**であったことです。\n",
    "> ここでいう線形関数は、$f(x + y) = f(x) + f(y)$を満たすような関数です。\n",
    "\n",
    "![linear](./images/linear.png)\n",
    "![non-linear](./images/non_linear.png)\n",
    "\n",
    "線形だと下のセルにある図1のような問題を解けません。図1にあるような問題を線形分離不可能と言います。\n",
    "このような問題は複数の直線か曲線でないと分離できません。一方、\n",
    "線形分離可能な問題というのはクラスタを直線で分離できるようなものです（fig 2.）。\n",
    "![fig 1](./images/fig_01.png)\n",
    "![fig 2](./images/fig_02.png)\n",
    "\n",
    "## 活性化関数\n",
    "ニューラルネットワークは非線形な関数ですが、シンプルには線形な関数を非線形にするために活性化関数を用います。\n",
    "活性化関数を適用する位置はスライド参照。  \n",
    "活性化関数はニューロンの発火にインスパイアされたもので、初めはステップ関数が使われていましたが、誤差を用いたパラメータ更新（学習）のことを\n",
    "考えて、微分可能な`sigmoid`関数が用いられるようになりました。現在では`ReLU`やその派生が主流になっています。\n",
    "`ReLU`が主流になっている理由としては、計算が速いことと微分が楽なことが挙げられます。  \n",
    "ほとんどの問題で、「線形変換によってデータの次元数を減らし、活性化関数を適用する」と言う処理を繰り返して出力します。\n",
    "![fig 3](./images/fig_03.png)\n",
    "![fig 4](./images/fig_04.png)\n",
    "![fig 5](./images/fig_05.png)\n",
    "![fig 6](./images/fig_06.png)\n",
    "\n",
    "## 誤差関数\n",
    "誤差関数は各タスクに合わせて設計されます。回帰問題では二乗誤差、識別問題では交差エントロピー（sigmoid cross entropy、softmax cross entropy）\n",
    "が主流です。  \n",
    "モデルの出力を$\\hat{\\boldsymbol{y}}$、正解を$\\boldsymbol{y}$とすると、\n",
    "二乗誤差は、$|| \\boldsymbol{y} - \\hat{\\boldsymbol{y}}||^2$、交差エントロピーは$ - \\boldsymbol{y}\\cdot \\log \\hat{\\boldsymbol{y}}$\n",
    "で計算できます。\n",
    "表記に関しては誤差の値自体は$\\bf{E}$、誤差関数は$L(\\hat{\\boldsymbol{y}}, \\boldsymbol{y}), \\ L$と表すことが多いです。\n",
    "\n",
    "![fig 7](./images/fig_07.png)\n",
    "\n",
    "普通、誤差は小さいほど良いです。　ただし、前回（4/16）に伝えた通り誤差は2種類あります。前回は図をお見せすることが\n",
    "できませんでしたが、下の図[^1]（青が訓練誤差、オレンジが汎化誤差を表す）では実際に初めは2つの誤差が\n",
    "順調に下がっていますが10epochあたりを境に汎化誤差は上がり始めています。そのため、このモデルは100epoch学習したもの\n",
    "よりも10epochあたりのものの方が未知のデータに対して精度が高いと想像できます。\n",
    "\n",
    "![overfitting](https://github.com/mitmul/chainer-handson/blob/master/DeepCNN_cifar10_result/loss.png?raw=true)\n",
    "\n",
    "[^1]: image from https://github.com/mitmul/chainer-handson\n",
    "\n",
    "### 誤差の計算について\n",
    "**訓練誤差、汎化誤差をどういうときに計算し、どのように言葉を使い分けるかについて。**\n",
    "訓練誤差と汎化誤差の計算方法は全く同じですがデータが*学習データかテストデータか*の違いがあります。\n",
    "また、誤差がパラメータの更新に使われるかどうかという大きな違いもあります。\n",
    "訓練誤差はパラメータ更新に使われますが、汎化誤差は学習の状況（過学習しているかなど）を見るためだけに計算します。\n",
    "そのため、多くのライブラリでモデルが学習用の計算かどうかをフラグで管理したり、変数が勾配を保持しないようにするためのフラグがあります。\n",
    "今回使うpytorchでは、モデルに対しては学習時には`model.train()`、評価（汎化誤差を計算する時や実際に運用する時）の時には`model.eval()`という\n",
    "メソッドが、変数クラス`Variable`には変数定義時に`Variable(data, volatile=True)`などと宣言します。\n",
    "\n",
    "## 誤差逆伝播\n",
    "偏微分・連鎖律がベースになっています。実際に扱う場合はフレームワークがよしなにやってくれているので特に説明しません。\n",
    "この誤差逆伝播で各レイヤーに誤差を渡し、その誤差を用いてパラメータを更新します。\n",
    "\n",
    "ここでは、PyTorchを用いて（toy dataで）誤差逆伝播をほぼ意識することなく実行できることを\n",
    "以下のコードを直下のセルにコピペして確認しましょう。`ctrl + b -> cmd + p -> shift + return`\n",
    "\n",
    "---\n",
    "\n",
    "```back_prop_check.py\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "\n",
    "x = Variable(torch.ones(2, 2), requires_grad=True)\n",
    "z = (x + 2) * (x + 2) * 3\n",
    "print('z: {}'.format(z))\n",
    "out = z.mean()\n",
    "print('out: {}'.format(out))\n",
    "out.backward()\n",
    "print(x.grad)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "上のsnippetでは\n",
    "${\\rm out} = \\dfrac{1}{4}\\sum_{i=1}^4 z_i =  \\dfrac{1}{4}\\sum_{i=1}^4 3 (x_i + 2)^2$より\n",
    "$\\dfrac{\\partial {\\rm out}}{\\partial x_i} = \\dfrac{\\partial {\\rm out}}{\\partial z_i} \\dfrac{\\partial z_i}{\\partial x_i} = \\dfrac{3}{2}(x_i + 2) |_{x_i = 1} = 4.5$\n",
    "が正解\n",
    "\n",
    "誤差逆伝播の確認↓"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ニューラルネット\n",
    "## 学習の流れ\n",
    "\n",
    "### ミニバッチ\n",
    "データセットの大きさを$N$とするとデータセットは教師あり学習の場合は$(\\boldsymbol{x}_i, \\boldsymbol{y}_i)_{i= 1}^N$と表せます。\n",
    "バッチ処理はデータセットのすべてのデータに対して誤差を計算し、パラメータを更新する方法ですが、$N = 10,000$を超えることがほとんどの場合、\n",
    "誤差を計算する回数に対してアップデートの回数が少なく、学習に時間がかかります。そのため、一般的には$n = 32$ぐらいのデータ（ミニバッチ）ごとに\n",
    "アップデートを行い、プログラム中ではミニバッチを1つの変数のように扱います。\n",
    "データの次元数が$d_{data}$の時、ミニバッチの形は$n \\times d_{data}$になります。\n",
    "\n",
    "よく、「学習は100epochで1epochは300iteration」などの言い方をしますが、\n",
    "データセットの大きさを$N$、ミニバッチの大きさを$n$とすると、イテレーション数は$N / n$となり、これが1epochに該当します。\n",
    "\n",
    "<!--\n",
    "## 実装\n",
    "コードを書いていきましょう。\n",
    "-->\n",
    "\n",
    "## データの扱い方\n",
    "ミニバッチを1つの変数として扱うので学習ループ内での各変数の形は`[batch_size, *data_shape]`になります。\n",
    "\n",
    "## モデルの構造\n",
    "モデルはレイヤー（線形変換）と活性化関数（非線形）で作られていますが、どちらも関数であることに変わりはないです。しかし、\n",
    "レイヤーは重みが更新されるので学習するにつれてその出力は変化します。`モデルの構造 1`では箱と箱を結ぶ線が活性化関数を、箱自体がレイヤーを表しています。また、`モデルの構造 2`では活性化関数が省略されています。  \n",
    "多くのライブラリではレイヤーと活性化関数のファイルのディレクトリが分けられています。\n",
    "\n",
    "###  レイヤー\n",
    "- Linear / Dense / Full Connected\n",
    "- Convolutional\n",
    "- Recurrent (LSTM)\n",
    "- Normalisation\n",
    "    - (Dropout)\n",
    "    - Batch Normalization\n",
    "    \n",
    "### 活性化関数\n",
    "- tanh\n",
    "- sigmoid\n",
    "- relu\n",
    "    - leaky relu\n",
    "    - prelu\n",
    "    - elu\n",
    "    \n",
    "## 多層パーセプトロン（Multi Layer Perceptron）\n",
    "### どんなモデルか\n",
    "多層パーセプトロンは`Linear`レイヤーのみからなるニューラルネットワークです。\n",
    "![mlp](./images/mat.png)\n",
    "各ユニットについての計算は上の図内にある通りですし、行列の大きさを（縦 $\\times$　横）で表すと、下の図のように行列の大きさを変更することができます。\n",
    "![matrix](./images/matrix.png)\n",
    "\n",
    "この性質を利用して、`Linear`レイヤーの計算ではミニバッチの計算を一度に行うことができます（下図）。\n",
    "`Linear`レイヤーを扱う場合、入力データの形は（ミニバッチサイズ、（データの形））となります。\n",
    "ここで説明した計算は、一度に複数の同じ形のニューラルネットワークの計算を行っているようなイメージです。\n",
    "\n",
    "![linear](./images/linear_layer.png)\n",
    "\n",
    "---\n",
    "\n",
    "ここでは、まず有名な手書き数字データセットのMNISTを用いて多層パーセプトロンの実験を行います。\n",
    "多層パーセプトロン（スライド参照）を実装しましょう。データセットはPyTorchに関数が用意されているのでそこから落とします。\n",
    "\n",
    "MNISTのデータは$0$から$9$の10クラス分類で各データは$28 \\times 28$の画像を下の図のように平らにした$784$次元です。\n",
    "![flatten_mnist](./images/flatten_image.png)\n",
    "この$784$次元のデータをいくつかの`Linear`を用いて$10$次元にしましょう。\n",
    "ここで使えるレイヤーは\n",
    "\n",
    "- `Linear`: 第1引数：入力データの次元数、第2引数：出力データの次元数\n",
    "- `BatchNorm1d`: 引数は次元数\n",
    "- `Dropout`: 引数は変数\n",
    "\n",
    "で、活性化関数は\n",
    "\n",
    "- `sigmoid`\n",
    "- `relu`\n",
    "- `tanh`\n",
    "\n",
    "などです。\n",
    "\n",
    "`import`する必要があるのは、\n",
    "\n",
    "```mnit_mlp.py\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.autograd import Variable\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import os  # path\n",
    "import datetime\n",
    "import numpy as np  # 行列演算\n",
    "import matplotlib.pyplot as plt  # グラフ\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "_cuda = False\n",
    "kwargs = {'num_workers': 1, 'pin_memory': True} if _cuda else {}\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('./', train=True, download=True,\n",
    "                   transform=transforms.ToTensor()),\n",
    "    batch_size=50, shuffle=True, **kwargs)\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('./', train=False, transform=transforms.ToTensor()),\n",
    "    batch_size=50, shuffle=True, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# モデルの定義\n",
    "class SimpleLinear(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_dim=784, out_dim=10, hidden_dim=100):\n",
    "        super(SimpleLinear, self).__init__()\n",
    "        self.fc1 = nn.Linear(in_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, out_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x.view(-1, 784)))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return F.log_softmax(x)\n",
    "    \n",
    "    def predict(self, x):\n",
    "        return nn.Softmax(self.forward(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SimpleLinearDr(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_dim=784, hidden_dim=256, out_dim=10):\n",
    "        super(SimpleLinearDr, self).__init__()\n",
    "        self.fc1 = nn.Linear(in_dim, hidden_dim)\n",
    "        self.dr1 = nn.Dropout()\n",
    "        self.fc2 = nn.Linear(hidden_dim, out_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.dr1(self.fc1(x.view(-1, 784))))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return F.log_softmax(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(epoch, log_interval=100):\n",
    "    model.train()  # モデルがロスを用いて更新するように設定\n",
    "    train_loss = 0\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = Variable(data), Variable(target)\n",
    "        optimizer.zero_grad()  # モデルのレイヤーが前のエポックでの勾配履歴を保持しているので消去\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        train_loss += loss.data[0]\n",
    "        loss.backward()  # 誤差逆伝播\n",
    "        optimizer.step()  # パラメータを更新\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print('Training')\n",
    "            print('epoch: {} [{}/{} ({:.0f}%)]\\tloss:{:.6f}'.format(epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                                                                   100. * batch_idx / len(train_loader), loss.data[0]))\n",
    "    return train_loss / len(train_loader)\n",
    "\n",
    "def test(epoch):\n",
    "    model.eval()  #  更新しないように設定\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    for data, target in test_loader:\n",
    "        data, target = Variable(data, volatile=True), Variable(target)\n",
    "        output = model(data)\n",
    "        test_loss += F.cross_entropy(output, target).data[0]\n",
    "        pred = output.data.max(1)[1]\n",
    "        correct += pred.eq(target.data).cpu().sum()\n",
    "\n",
    "    test_loss /= len(test_loader)\n",
    "    print('Test')\n",
    "    print('Average loss: {:.4f},\\tAccuracy: {}/{} ({:.0f}%)'.format(test_loss, correct, len(test_loader.dataset),\n",
    "                                                                   100. * correct / len(test_loader.dataset)))\n",
    "    return test_loss, correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run(n_epoch, train, test):\n",
    "    train_loss = []\n",
    "    test_loss = []\n",
    "    test_accuracy = []\n",
    "    start = datetime.datetime.now()\n",
    "    print('Training started at {}'.format(start.strftime('%Y-%m-%d, %H:%M')))\n",
    "    for epoch in range(1, n_epoch + 1):\n",
    "        if epoch == 1:\n",
    "            loop_start = datetime.datetime.now()\n",
    "        train_loss.append(train(epoch))\n",
    "        t_loss, t_accuracy = test(epoch)\n",
    "        test_loss.append(t_loss)\n",
    "        test_accuracy.append(t_accuracy)\n",
    "        if epoch == 1:\n",
    "            loop_end = datetime.datetime.now()\n",
    "            print('one epoch takes {} sec'.format((loop_end - loop_start).total_seconds()))\n",
    "    end = datetime.datetime.now()\n",
    "    print('Training finished at {}'.format(end.strftime('%Y-%m-%d, %H:%M')))\n",
    "    print('Duration: {}'.format((end - start).total_seconds()))\n",
    "    return train_loss, test_loss, test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_results(train_loss, test_loss, test_accuracy=None, save=False, save_dir='./images/result'):\n",
    "    time_stamp = datetime.datetime.now().strftime('%Y/%m/%d, %H:%M')\n",
    "    f_time = datetime.datetime.now().strftime('%Y_%m_%d_%H_%M')\n",
    "    plt.plot(list(range(1, len(train_loss) + 1)), train_loss, label='train loss')\n",
    "    plt.plot(list(range(1, len(test_loss) + 1)), test_loss, label='test loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('loss')\n",
    "    plt.grid()\n",
    "    plt.title('loss result at {}'.format(time_stamp))\n",
    "    plt.legend()\n",
    "    if save:\n",
    "        tmp_save_dir = os.path.join(save_dir, 'loss')\n",
    "        f_name = os.path.join(tmp_save_dir, '{}.png'.format(f_time))\n",
    "        if not os.path.exists(tmp_save_dir):\n",
    "            os.makedirs(tmp_save_dir)\n",
    "        print('path to save loss: {}'.format(f_name))\n",
    "        plt.savefig(f_name)\n",
    "    plt.show()\n",
    "    \n",
    "    if test_accuracy is not None:\n",
    "        plt.plot(list(range(1, len(test_accuracy) + 1)), list(np.asarray(test_accuracy) / 100.0), label='test accuracy')\n",
    "        plt.xlabel('epoch')\n",
    "        plt.ylabel('accuracy')\n",
    "        plt.grid()\n",
    "        plt.title('accuracy result at {}'.format(time_stamp))\n",
    "        plt.legend()\n",
    "        if save:\n",
    "            tmp_save_dir = os.path.join(save_dir, 'accuracy')\n",
    "            f_name = os.path.join(tmp_save_dir, '{}.png'.format(f_time))\n",
    "            if not os.path.exists(tmp_save_dir):\n",
    "                os.makedirs(tmp_save_dir)\n",
    "            print('path to save accuracy: {}'.format(f_name))\n",
    "            plt.savefig(f_name)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ミニマリズムなパーセプトロン"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = SimpleLinear()\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "train_loss, test_loss, test_accuracy = run(100, train, test)\n",
    "plot_results(train_loss, test_loss, test_accuracy, save=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### optimizerの違い"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = SimpleLinear()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1, momentum=0.9)\n",
    "optim_results = run(100, train, test)\n",
    "plot_results(*optim_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropoutを追加"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = SimpleLinearDr()\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "dr_results = run(100, train, test)\n",
    "plot_results(*dr_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ComplexLinear(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_size=784, hidden_1=256, hidden_2=128, hidden_3=64, out_size=10, dr=True):\n",
    "        super(ComplexLinear, self).__init__()\n",
    "        self.dr_flag = dr\n",
    "        self.fc1 = nn.Linear(in_size, hidden_1)\n",
    "        if dr:\n",
    "            self.dr1 = nn.Dropout()\n",
    "        self.fc2 = nn.Linear(hidden_1, hidden_2)\n",
    "        if dr:\n",
    "            self.dr2 = nn.Dropout()\n",
    "        self.fc3 = nn.Linear(hidden_2, hidden_3)\n",
    "        if dr:\n",
    "            self.dr3 = nn.Dropout()\n",
    "        self.fc4 = nn.Linear(hidden_3, out_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        if self.dr_flag:\n",
    "            h1 = F.relu(self.dr1(self.fc1(x.view(-1, 784))))\n",
    "            h2 = F.relu(self.dr2(self.fc2(h1)))\n",
    "            h3 = F.relu(self.dr3(self.fc3(h2)))\n",
    "        else:\n",
    "            h1 = F.relu(self.fc1(x.view(-1, 784)))\n",
    "            h2 = F.relu(self.fc2(h1))\n",
    "            h3 = F.relu(h2)\n",
    "        h4 = self.fc4(h3)\n",
    "        return F.log_softmax(h4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# complex model\n",
    "model = ComplexLinear()\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "complex_w_dr_results = run(100, train, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plot_results(*complex_w_dr_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = SimpleLinearDr()\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "results_dr = run(100, train, test)\n",
    "plot_results(*results_dr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# オートエンコーダー\n",
    "前回、あんなにオートエンコーダーの話をしたのに、ここでしないわけにいきません。\n",
    "オートエンコーダでは、データの大きさは砂時計のように入出力時が大きく、中間で一番小さくなるように設計します。ここでも、各層での次元数は任意です。今回はクラス数に合わせて、$10$次元に潰してみましょう\n",
    "\n",
    "![autoencoder](./images/autoencoder.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class AutoEncoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_size=784, hidden_size=256, latent_size=10):\n",
    "        super(AutoEncoder, self).__init__()\n",
    "        self.fc1 = nn.Linear(in_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, latent_size)\n",
    "        self.fc3 = nn.Linear(latent_size, hidden_size)\n",
    "        self.fc4 = nn.Linear(hidden_size, in_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        h = F.relu(self.fc1(x.view(-1, 784)))\n",
    "        h = F.relu(self.fc2(h))\n",
    "        h = F.relu(self.fc3(h))\n",
    "        return self.fc4(h)\n",
    "    \n",
    "    def infer(self, x):\n",
    "        h = F.relu(self.fc1(x.view(-1, 784)))\n",
    "        return self.fc2(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class AutoEncoderDR(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_size=784, hidden_size=256, latent_size=10):\n",
    "        super(AutoEncoderDR, self).__init__()\n",
    "        self.fc1 = nn.Linear(in_size, hidden_size)\n",
    "        self.dr1 = nn.Dropout()\n",
    "        self.fc2 = nn.Linear(hidden_size, latent_size)\n",
    "        self.fc3 = nn.Linear(latent_size, hidden_size)\n",
    "        self.dr3 = nn.Dropout()\n",
    "        self.fc4 = nn.Linear(hidden_size, in_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 784)\n",
    "        for i in range(1, 4):\n",
    "            x = getattr(self, 'fc{}'.format(i))(x)\n",
    "            if i != 2:\n",
    "                x = getattr(self, 'dr{}'.format(i))(x)\n",
    "            x = F.sigmoid(x)\n",
    "        return self.fc4(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_ae(epoch, ae, log_interval=100):\n",
    "    ae.train()\n",
    "    train_loss = .0\n",
    "    for batch_idx, (data, _) in enumerate(train_loader):\n",
    "        data = Variable(data)\n",
    "        output = ae(data)\n",
    "        reconstruction_loss = (output - data) * (output - data)\n",
    "        reconstruction_loss = reconstruction_loss.mean()\n",
    "        train_loss += reconstruction_loss.data[0]\n",
    "        reconstruction_loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print('Training')\n",
    "            print('epoch: {} [{}/{} ({:.0f}%)]\\tloss:{:.6f}'.format(epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                                                                   100. * batch_idx / len(train_loader), reconstruction_loss.data[0]))\n",
    "    return train_loss / len(train_loader)\n",
    "\n",
    "def test_ae(epoch, ae):\n",
    "    ae.eval()\n",
    "    test_loss = .0\n",
    "    for (data, _) in test_loader:\n",
    "        data = Variable(data, volatile=True)\n",
    "        output = ae(data)\n",
    "        reconstruction_loss = (output - data) * (output - data)\n",
    "        test_loss += reconstruction_loss.mean().data[0]\n",
    "    test_loss /= len(test_loader)\n",
    "    print('Test')\n",
    "    print('Average loss: {:.4f}'.format(test_loss))\n",
    "    return test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_ae(n_epoch, train_ae, test_ae):\n",
    "    train_loss = []\n",
    "    test_loss = []\n",
    "    start_time = datetime.datetime.now()\n",
    "    f_start = start_time.strftime('%H%M')\n",
    "    print('Training AutoEncoder started at {}'.format(start_time.strftime('%Y/%m/%d, %H:%M')))\n",
    "    for epoch in range(1, n_epoch + 1):\n",
    "        if epoch == 1:\n",
    "            start = datetime.datetime.now()\n",
    "        train_loss.append(train_ae(epoch, ae))\n",
    "        test_loss.append(test_ae(epoch, ae))\n",
    "        if epoch == 1:\n",
    "            end = datetime.datetime.now()\n",
    "            print('# one loop takes {} sec'.format((end - start).total_seconds()))\n",
    "    end_time = datetime.datetime.now()\n",
    "    print('Training AutoEncoder finishied at {}'.format(end_time.strftime('%Y/%m/%d, %H:%M')))\n",
    "    print('Duration: {} sec'.format((end_time - start_time).total_seconds()))\n",
    "    return train_loss, test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ae = AutoEncoder()\n",
    "optimizer = optim.Adam(ae.parameters())\n",
    "results_ae = run_ae(50, train_ae, test_ae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ae_bn = AutoEncoderBN()\n",
    "optimizer = optim.Adam(ae.parameters())\n",
    "results_ae = run_ae(100, train_ae, test_ae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ae_dr = AutoEncoderDR()\n",
    "optimizer = optim.Adam(ae.parameters())\n",
    "results_ae = run_ae(10, train_ae, test_ae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calculate_latent(ae):\n",
    "    ae.eval()\n",
    "    x0_list = []\n",
    "    x1_list = []\n",
    "    label_list = [] \n",
    "    for (data, target) in test_loader:\n",
    "        data = Variable(data, volatile=True)\n",
    "        latent = ae.infer(data).data.numpy().transpose(1, 0)\n",
    "        x0_list.extend(list(latent[0, :]))\n",
    "        x1_list.extend(list(latent[0, :]))\n",
    "        label_list.extend(list(target))\n",
    "    return pd.DataFrame({'x0': x0_list, 'x1': x1_list, 'digit': label_list})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def reconstruct_mnist(ae):\n",
    "    ae.eval()\n",
    "    original_data = []\n",
    "    reconstruncted = []\n",
    "    digit_list = []\n",
    "    for (data, target) in test_loader:\n",
    "        data = Variable(data, volatile=True)\n",
    "        reconstruncted_data = np.transpose(ae(data).view(-1, 1, 28, 28).data.numpy(), (0, 2, 3, 1))\n",
    "        original_data.extend(list(data.data.numpy().transpose(0, 2, 3, 1)))\n",
    "        reconstruncted.extend(list(reconstruncted_data))\n",
    "        digit_list.extend(list(target))\n",
    "    return original_data, reconstruncted, digit_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(type(result_ae))\n",
    "print(type(result_ae[0]))\n",
    "print(type(result_ae[0][0]))\n",
    "print(result_ae[0][0].shape)\n",
    "\n",
    "plt.imshow(result_ae[0][0][:, :, 0], cmap='gray')\n",
    "plt.show()\n",
    "\n",
    "plt.imshow(result_ae[1][0][:, :, 0], cmap='gray')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
